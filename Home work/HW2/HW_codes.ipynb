{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       species  class_encoded\n",
      "0       Adelie              0\n",
      "1       Adelie              0\n",
      "2       Adelie              0\n",
      "4       Adelie              0\n",
      "5       Adelie              0\n",
      "..         ...            ...\n",
      "215  Chinstrap              1\n",
      "216  Chinstrap              1\n",
      "217  Chinstrap              1\n",
      "218  Chinstrap              1\n",
      "219  Chinstrap              1\n",
      "\n",
      "[214 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "# Display the filtered and encoded DataFrame\n",
    "print(df_filtered[['species', 'class_encoded']])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "X = df_filtered.drop(['species', 'island', 'sex','class_encoded'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1 What is the purpose of \"y_encoded = le.fit_transform(df_filtered['species'])\" ?\n",
    "\n",
    "2 What is the purpose of \"X = df.drop(['species', 'island', 'sex'], axis=1)\" ?\n",
    "\n",
    "3 Why we cannot use \"island\" and \"sex\" features?\n",
    "\n",
    "---\n",
    "\n",
    "1. The purpose of the line `y_encoded = le.fit_transform(df_filtered['species'])` is to encode the target variable `('species')` into numerical values. In many machine learning algorithms, the target variable should be numerical, not categorical. The LabelEncoder from scikit-learn is used to convert the species names (categorical labels) into unique integer labels. This encoding allows you to use the 'species' column as the target variable in machine learning models like logistic regression, where it requires numerical inputs for classification.\n",
    "\n",
    "\n",
    "2. The purpose of the line X = df.drop(['species', 'island', 'sex'], axis=1) is to create a feature matrix X by removing columns from the original DataFrame df that are not intended to be used as features in a machine learning model. In this line, the columns 'species', 'island', and 'sex' are dropped from the DataFrame, leaving only the columns that are considered as features for training the model. This step is essential because machine learning models typically require numerical features, and certain columns like 'species', 'island', and 'sex' are categorical and need preprocessing before they can be used as features.\n",
    "\n",
    "3. \n",
    "'Island' is a categorical feature representing the island on which a penguin was observed. To use it as a feature, you would typically perform one-hot encoding or another form of categorical encoding to convert it into numerical values. \n",
    "\n",
    "'Sex' is another categorical feature representing the gender of the penguin. Similar to 'island', we would need to perform encoding to convert it into numerical values. The choice of encoding method (e.g., one-hot encoding or label encoding) depends on the specific modeling approach we use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5813953488372093\n",
      "[[ 2.75700244e-03 -8.37987445e-05  4.49753068e-04 -2.85861156e-04]] [-8.54905722e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojitha Mirihagalla\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Why is accuracy low? why does the saga solver perform poorly?\n",
    "\n",
    "The low accuracy could be due to various reasons such as data quality, imbalanced classes, and model complexity. The 'saga' solver might not perform optimally for your specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "[[ 1.59665154 -1.42501103 -0.15238046 -0.003951  ]] [-0.0755452]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='liblinear')\n",
    "logreg.fit(X_train, y_train)\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is accuracy now? why does the \"liblinear\" solver perform better than \"saga\" solver ?\n",
    "\n",
    "\n",
    "The \"liblinear\" solver may perform better than \"saga\" due to factors like dataset size, regularization strength, and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 'saga' solver and scaled features: 0.9767441860465116\n",
      "Accuracy with 'liblinear' solver and unscaled features: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression with 'saga' solver and scaled features\n",
    "logreg_saga = LogisticRegression(solver='saga', random_state=42)\n",
    "logreg_saga.fit(X_train_scaled, y_train)\n",
    "y_pred_saga = logreg_saga.predict(X_test_scaled)\n",
    "accuracy_saga = accuracy_score(y_test, y_pred_saga)\n",
    "print(\"Accuracy with 'saga' solver and scaled features:\", accuracy_saga)\n",
    "\n",
    "# Logistic Regression with 'liblinear' solver and unscaled features\n",
    "logreg_liblinear = LogisticRegression(solver='liblinear', random_state=42)\n",
    "logreg_liblinear.fit(X_train, y_train)\n",
    "y_pred_liblinear = logreg_liblinear.predict(X_test)\n",
    "accuracy_liblinear = accuracy_score(y_test, y_pred_liblinear)\n",
    "print(\"Accuracy with 'liblinear' solver and unscaled features:\", accuracy_liblinear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the accuracies for both  \"liblinear\" solver and \"saga\" solver. Why accuracy of the \"saga\" solver is increased?\n",
    "\n",
    "Now it has been increased in accuracy for the \"saga\" solver to 97% may be due to the benefits of feature scaling, which helps the solver converge to a better solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5813953488372093\n",
      "[[-2.47386886e-05  2.75968297e-03 -8.11810431e-05  4.65073280e-04\n",
      "  -2.86632956e-04  1.00269118e-05]] [-8.42137777e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojitha Mirihagalla\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "\n",
    "df_filtered.head()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "#encode sex column\n",
    "le_1 = LabelEncoder()\n",
    "\n",
    "sex_encoded = le_1.fit_transform(df_filtered['sex'])\n",
    "df_filtered.drop(['sex'], axis=1)\n",
    "df_filtered['sex'] = sex_encoded\n",
    "\n",
    "#encode island column\n",
    "le_2 = LabelEncoder()\n",
    "\n",
    "island_encoded = le_2.fit_transform(df_filtered['island'])\n",
    "df_filtered.drop(['island'], axis=1)\n",
    "df_filtered['island'] = island_encoded\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "\n",
    "X = df_filtered.drop(['species', 'class_encoded'], axis=1)  # Choose features\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "\n",
    "X.head()\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the problem? Why algorithm cannot perform classification?\n",
    "\n",
    "beacuse, there are two non numerical data sets in the input data frame. So, it can't be usede to train the model when those are in string type.\n",
    "\n",
    "2. How to solve this issue?\n",
    "\n",
    "we can either remove or map that data to numberical value. Then we can run the model. But According to the observation 'Sex' and 'island' data sets don't give much impact to training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5813953488372093\n",
      "[[ 2.76514270e-03 -8.25155584e-05  4.71591095e-04 -2.87125287e-04\n",
      "   1.84941694e-04 -1.05141515e-04  1.05127125e-05]] [-8.48042914e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojitha Mirihagalla\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#encode string columns into numeric columns\n",
    "# logistic regression\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the penguins dataset\n",
    "df = sns.load_dataset(\"penguins\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Filter rows for 'Adelie' and 'Chinstrap' classes\n",
    "selected_classes = ['Adelie', 'Chinstrap']\n",
    "df_filtered = df[df['species'].isin(selected_classes)].copy()  # Make a copy to avoid the warning\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode the species column\n",
    "y_encoded = le.fit_transform(df_filtered['species'])\n",
    "df_filtered['class_encoded'] = y_encoded\n",
    "\n",
    "df_filtered = pd.get_dummies(df_filtered, columns=['island', 'sex'], drop_first=True)\n",
    "df_filtered.head()\n",
    "\n",
    "X = df_filtered.drop(['species', 'class_encoded'], axis=1)  # Choose features\n",
    "y = df_filtered['class_encoded']  # Target variable\n",
    "\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0  Adelie            39.1           18.7              181.0       3750.0   \n",
      "1  Adelie            39.5           17.4              186.0       3800.0   \n",
      "\n",
      "   class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0              0         False              True      True  \n",
      "1              0         False              True     False  \n",
      "\n",
      "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0   Adelie            39.1           18.7              181.0       3750.0   \n",
      "20  Adelie            37.8           18.3              174.0       3400.0   \n",
      "\n",
      "    class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0               0         False              True      True  \n",
      "20              0         False             False     False  \n",
      "\n",
      "   species  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0   Adelie            39.1           18.7              181.0       3750.0   \n",
      "30  Adelie            39.5           16.7              178.0       3250.0   \n",
      "\n",
      "    class_encoded  island_Dream  island_Torgersen  sex_Male  \n",
      "0               0         False              True      True  \n",
      "30              0          True             False     False  \n"
     ]
    }
   ],
   "source": [
    "# visualize the data\n",
    "samples = df_filtered.groupby('sex_Male').head(1)\n",
    "print(samples)\n",
    "print()\n",
    "samples = df_filtered.groupby('island_Torgersen').head(1)\n",
    "print(samples)\n",
    "print()\n",
    "samples = df_filtered.groupby('island_Dream').head(1)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 7) (214,)\n",
      "Accuracy: 1.0\n",
      "[[ 3.63424554  0.16322352  0.6264067   0.10226686  2.59927497 -0.87716064\n",
      "  -0.35907824]] [-5.99663294]\n"
     ]
    }
   ],
   "source": [
    "y = df_filtered['class_encoded']  # Target variable\n",
    "print(X.shape, y.shape)\n",
    "X.head()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler=MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(solver='saga',max_iter=150,)\n",
    "\n",
    "#logreg = LogisticRegression(max_iter=166, solver='newton-cg')\n",
    "# logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100, multi_class='ovr', random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why we are using the \"MaxAbsScaler\" scaler rather than the \"StandardScaler\"?\n",
    "\n",
    "In MaxAbsScaler we map each value of data set into values between -1  and 1. So, this is a fix range scaling method relative to the standard scaler. Here, we don't want to map the values to normal distribution. Beacuse we don't want to change the shape of distribution. Therefore, we use MaxAbs Scaler rather than using Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59655172 0.98139535 0.93396226 0.91666667 0.         1.\n",
      "  1.        ]\n",
      " [0.8862069  0.88372093 0.94811321 0.82291667 1.         0.\n",
      "  1.        ]\n",
      " [0.68275862 0.8        0.9245283  0.73958333 0.         1.\n",
      "  0.        ]\n",
      " [0.87586207 0.86046512 0.94811321 0.92708333 1.         0.\n",
      "  1.        ]\n",
      " [0.7137931  0.86046512 0.95283019 0.80729167 0.         1.\n",
      "  1.        ]\n",
      " [0.64310345 0.95348837 0.93867925 0.78645833 0.         1.\n",
      "  1.        ]\n",
      " [0.65172414 0.85116279 0.82075472 0.70833333 0.         0.\n",
      "  0.        ]\n",
      " [0.5862069  0.79534884 0.87264151 0.70833333 1.         0.\n",
      "  0.        ]\n",
      " [0.73965517 0.81860465 0.9245283  0.97916667 0.         1.\n",
      "  1.        ]\n",
      " [0.62068966 0.79534884 0.88207547 0.77083333 1.         0.\n",
      "  0.        ]\n",
      " [0.82068966 0.85116279 0.91981132 0.80208333 1.         0.\n",
      "  0.        ]\n",
      " [0.80517241 0.83255814 0.91981132 0.6875     1.         0.\n",
      "  0.        ]\n",
      " [0.63103448 0.85581395 0.86792453 0.72395833 1.         0.\n",
      "  0.        ]\n",
      " [0.72586207 0.88837209 0.91981132 0.83333333 0.         1.\n",
      "  1.        ]\n",
      " [0.73275862 0.77674419 0.88207547 0.69791667 1.         0.\n",
      "  0.        ]\n",
      " [0.8362069  0.81395349 0.9009434  0.70833333 1.         0.\n",
      "  1.        ]\n",
      " [0.87758621 0.83255814 0.9245283  0.765625   1.         0.\n",
      "  0.        ]\n",
      " [0.65862069 0.84186047 0.87264151 0.82291667 0.         0.\n",
      "  1.        ]\n",
      " [0.69137931 0.87906977 0.88679245 0.89583333 0.         0.\n",
      "  1.        ]\n",
      " [0.80862069 0.77209302 0.90566038 0.5625     1.         0.\n",
      "  0.        ]\n",
      " [0.65344828 0.86511628 0.91037736 0.609375   0.         0.\n",
      "  0.        ]\n",
      " [0.96206897 0.92093023 0.97641509 0.83333333 1.         0.\n",
      "  1.        ]\n",
      " [0.85517241 0.84651163 0.91037736 0.78645833 1.         0.\n",
      "  1.        ]\n",
      " [0.85862069 0.80465116 0.93396226 0.765625   1.         0.\n",
      "  0.        ]\n",
      " [0.88448276 0.89302326 0.91037736 0.76041667 1.         0.\n",
      "  1.        ]\n",
      " [0.65689655 0.76744186 0.93396226 0.796875   0.         0.\n",
      "  0.        ]\n",
      " [0.67586207 0.98139535 0.9245283  0.86458333 1.         0.\n",
      "  1.        ]\n",
      " [0.68103448 0.77674419 0.83962264 0.67708333 1.         0.\n",
      "  0.        ]\n",
      " [0.91034483 0.93023256 0.96698113 0.94791667 1.         0.\n",
      "  1.        ]\n",
      " [0.65       0.86976744 0.8490566  0.75       0.         0.\n",
      "  1.        ]\n",
      " [0.71206897 0.98139535 0.91981132 0.91666667 0.         0.\n",
      "  1.        ]\n",
      " [0.64310345 0.78139535 0.90566038 0.625      1.         0.\n",
      "  0.        ]\n",
      " [0.81034483 0.80465116 0.87264151 0.77083333 1.         0.\n",
      "  0.        ]\n",
      " [0.74482759 0.86046512 0.90566038 0.85416667 1.         0.\n",
      "  1.        ]\n",
      " [0.73793103 0.86046512 0.91981132 0.88541667 0.         1.\n",
      "  1.        ]\n",
      " [0.85       0.9255814  0.95754717 0.84375    1.         0.\n",
      "  1.        ]\n",
      " [0.89655172 0.88372093 0.92924528 0.86458333 1.         0.\n",
      "  1.        ]\n",
      " [0.79482759 0.84651163 0.83962264 0.67708333 1.         0.\n",
      "  0.        ]\n",
      " [0.65172414 0.93023256 0.89622642 0.88541667 0.         0.\n",
      "  1.        ]\n",
      " [0.61551724 0.8372093  0.95283019 0.73958333 1.         0.\n",
      "  0.        ]\n",
      " [0.65862069 0.93023256 0.89622642 0.8125     0.         0.\n",
      "  1.        ]\n",
      " [0.73275862 0.80465116 0.88207547 0.69791667 1.         0.\n",
      "  0.        ]\n",
      " [0.61206897 0.75348837 0.91981132 0.69791667 0.         0.\n",
      "  0.        ]]\n",
      "[[-1.34082659  2.35505035  0.88068888  1.62029553 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 1.80078227  0.5480021   1.30057122  0.57562238  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.40582395 -1.0008964   0.60076732 -0.35297599 -1.14490646  1.80969611\n",
      "  -0.98260737]\n",
      " [ 1.68858195  0.11775252  1.30057122  1.73637033  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.069223    0.11775252  1.440532    0.40151018 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-0.83592516  1.83875085  1.02064966  0.16936059 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-0.7424249  -0.05434732 -2.47836983 -0.70120037 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [-1.4530269  -1.08694632 -0.93880125 -0.70120037  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.21127779 -0.65669673  0.60076732  2.3167443  -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [-1.07902585 -1.08694632 -0.65887969 -0.0047516   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.09018027 -0.05434732  0.46080654  0.34347279  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.92187979 -0.39854698  0.46080654 -0.93334996  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.96682553  0.0317026  -1.07876203 -0.52708818  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.06167737  0.63405202  0.46080654  0.69169717 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 0.13647758 -1.43114598 -0.65887969 -0.81727517  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.25848074 -0.74274665 -0.09903657 -0.70120037  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.70728201 -0.39854698  0.60076732 -0.062789    0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.66762469 -0.22644715 -0.93880125  0.57562238 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.31232369  0.46195218 -0.51891891  1.38814594 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.9592799  -1.5171959   0.0409242  -2.3262475   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.72372485  0.20380243  0.18088498 -1.80391093 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [ 2.62358459  1.23640143  2.1403359   0.69169717  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.46418132 -0.14039723  0.18088498  0.16936059  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.50158143 -0.91484648  0.88068888 -0.062789    0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 1.78208222  0.72010193  0.18088498 -0.1208264   0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.68632474 -1.60324582  0.88068888  0.28543539 -1.14490646 -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.48062416  2.35505035  0.60076732  1.03992156  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.424524   -1.43114598 -1.91852671 -1.04942476  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 2.06258301  1.40850127  1.86041434  1.96851992  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [-0.76112495  0.28985235 -1.63860515 -0.23690119 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.08792305  2.35505035  0.46080654  1.62029553 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-0.83592516 -1.34509607  0.0409242  -1.62979873  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.97797995 -0.91484648 -0.93880125 -0.0047516   0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [ 0.26737795  0.11775252  0.0409242   0.92384676  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.19257774  0.11775252  0.46080654  1.27207115 -1.14490646  1.80969611\n",
      "   1.01770049]\n",
      " [ 1.40808116  1.32245135  1.58049278  0.80777197  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 1.91298259  0.5480021   0.7407281   1.03992156  0.8734338  -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.80967948 -0.14039723 -1.91852671 -1.04942476  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.7424249   1.40850127 -0.23899735  1.27207115 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [-1.13512601 -0.31249707  1.440532   -0.35297599  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-0.66762469  1.40850127 -0.23899735  0.45954758 -1.14490646 -0.55257896\n",
      "   1.01770049]\n",
      " [ 0.13647758 -0.91484648 -0.65887969 -0.81727517  0.8734338  -0.55257896\n",
      "  -0.98260737]\n",
      " [-1.17252611 -1.86139557  0.46080654 -0.81727517 -1.14490646 -0.55257896\n",
      "  -0.98260737]]\n"
     ]
    }
   ],
   "source": [
    "# visualize the scaled data\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler=MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What can you observe in the values related to \"island_Dream\",    \"island_Torgersen\"  and   \"sex_Male\" features before and after scaling?\n",
    "\n",
    "before scaling we have boolean values as True or False , After scaling we can see, those values have been replaced with binary values . \n",
    "\n",
    "True  - > 1  , False - > 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
